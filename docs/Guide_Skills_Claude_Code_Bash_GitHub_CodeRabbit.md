# **Guide Complet : Cr√©er des Skills Claude Code Bas√©s sur Bash pour l'Analyse des Pull Requests GitHub avec CodeRabbit**

## **Table des Mati√®res**

1. [Introduction et Vision du Projet](#introduction-et-vision-du-projet)
2. [Architecture Globale du Syst√®me](#architecture-globale-du-syst√®me)
3. [Pr√©requis et Installation](#pr√©requis-et-installation)
4. [Structure des Skills Claude Code](#structure-des-skills-claude-code)
5. [Skill 1: GitHub PR Collector](#skill-1-github-pr-collector)
6. [Skill 2: CodeRabbit Review Analyzer](#skill-2-coderabbit-review-analyzer)
7. [Scripts Bash S√©curis√©s](#scripts-bash-s√©curis√©s)
8. [Installation et D√©ploiement](#installation-et-d√©ploiement)
9. [Utilisation et Workflows](#utilisation-et-workflows)
10. [Maintenance et √âvolution](#maintenance-et-√©volution)

---

## **Introduction et Vision du Projet**

### **Contexte et Objectifs**

Dans l'√©cosyst√®me moderne de d√©veloppement logiciel, les Pull Requests constituent le point n√©vralgique de la collaboration et de la qualit√© du code. Les outils d'IA comme **CodeRabbit** enrichissent consid√©rablement ce processus en fournissant des analyses automatis√©es sophistiqu√©es, mais l'exploitation optimale de ces donn√©es n√©cessite une orchestration intelligente.

Ce guide pr√©sente une approche innovante combinant :
- **Les Skills Claude Code** pour encapsuler l'expertise d'analyse
- **Des scripts Bash s√©curis√©s** pour √©conomiser les tokens et optimiser les performances
- **GitHub CLI** pour une int√©gration native avec l'√©cosyst√®me GitHub
- **L'extraction jq** pour un parsing pr√©cis des m√©tadonn√©es CodeRabbit

### **Philosophie : Bash-First avec Intelligence IA**

L'approche propos√©e suit le principe **"Bash-First, IA-Smart"** :
- Les t√¢ches d√©terministes (r√©cup√©ration de donn√©es, parsing, filtrage) sont d√©l√©gu√©es √† Bash
- L'intelligence artificielle se concentre sur l'analyse, la synth√®se et la g√©n√©ration de insights
- Cette r√©partition optimise l'utilisation des tokens tout en maximisant la fiabilit√©

---

## **Architecture Globale du Syst√®me**

### **Vue d'Ensemble des Composants**

```mermaid
graph TD
    A[GitHub Repository] -->|GitHub CLI| B[PR Data Collection]
    B --> C[jq Processing]
    C --> D[.scd Storage]
    D --> E[Claude Skills]
    E --> F[Analysis & Reports]
    
    subgraph "Skills Claude Code"
        E1[GitHub PR Collector]
        E2[CodeRabbit Review Analyzer]
    end
    
    subgraph "Data Pipeline"
        B1[GitHub CLI Scripts]
        C1[jq Parsing Scripts]
        D1[Markdown Generation]
    end
```

### **Flux de Donn√©es**

1. **Collection** : GitHub CLI r√©cup√®re les donn√©es des PR en cours
2. **Extraction** : jq parse les commentaires CodeRabbit et extrait les m√©tadonn√©es
3. **Classification** : Les commentaires sont tri√©s par type et importance
4. **Stockage** : G√©n√©ration de fichiers Markdown structur√©s dans `.scd/`
5. **Analyse** : Claude Code utilise les donn√©es pr√©process√©es pour l'analyse avanc√©e

### **Structure du Projet Utilisateur**

```
your-project/                      # Projet de l'utilisateur
‚îú‚îÄ‚îÄ .claude/                       # Skills Claude Code (install√©s localement)
‚îÇ   ‚îî‚îÄ‚îÄ skills/
‚îÇ       ‚îú‚îÄ‚îÄ github-pr-collector/
‚îÇ       ‚îî‚îÄ‚îÄ review-analyzer/
‚îú‚îÄ‚îÄ .scd/                          # Donn√©es des analyses (dans le projet)
‚îÇ   ‚îú‚îÄ‚îÄ pr-data/                   # Donn√©es des Pull Requests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pr-{number}/           # Dossier par PR
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üî¥-critical/       # Commentaires critiques
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ comment-{id}.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comment-{id}.md
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üü†-major/          # Commentaires majeurs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üü°-minor/          # Commentaires mineurs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üîµ-trivial/        # Commentaires triviaux
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary.md         # R√©sum√© de la PR
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ global-summary.md      # R√©sum√© global
‚îÇ   ‚îú‚îÄ‚îÄ config/                    # Configuration des agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents-patterns.json   # Patterns pour tous les agents
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ severity-mapping.json  # Mapping de s√©v√©rit√©
‚îÇ   ‚îî‚îÄ‚îÄ cache/                     # Cache temporaire
‚îî‚îÄ‚îÄ your-code/                     # Code du projet utilisateur
    ‚îú‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ README.md
```

---

## **Pr√©requis et Installation**

### **D√©pendances Syst√®me**

```bash
# Outils requis
- GitHub CLI (gh) >= 2.0.0
- jq >= 1.6
- curl
- bash >= 4.0
```

### **Configuration GitHub CLI**

```bash
# Authentification GitHub CLI (une fois)
gh auth login

# V√©rification des permissions
gh auth status

# Test de base
gh pr list --repo $(gh repo view --json nameWithOwner -q .nameWithOwner)
```

### **Variables d'Environnement**

```bash
# Variables automatiquement d√©tect√©es dans le projet courant
CC_SKILLS_PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
CC_SKILLS_DATA_DIR="${CC_SKILLS_PROJECT_ROOT}/.scd"
CC_SKILLS_CONFIG_DIR="${CC_SKILLS_DATA_DIR}/config"
```

---

## **Structure des Skills Claude Code**

### **Skill 1: GitHub PR Collector**

#### **SKILL.md - GitHub PR Collector**

```yaml
---
name: "github-pr-collector"
description: "Collecte et analyse les Pull Requests GitHub avec leurs commentaires d'agents de review IA (CodeRabbit, GitHub Copilot, Codex, etc.). Utilise GitHub CLI pour r√©cup√©rer les donn√©es, extrait les m√©tadonn√©es des agents avec jq, et g√©n√®re une structure organis√©e par PR et par importance dans le dossier .scd du projet. Extensible pour supporter de nouveaux agents de review."
version: "1.0.0"
dependencies:
  - "github-cli >= 2.0.0"
  - "jq >= 1.6"
---

# GitHub PR Collector Skill

## Objectif

Ce skill automatise la collecte et l'extraction des donn√©es des Pull Requests GitHub, avec support pour multiple agents de review IA (CodeRabbit, GitHub Copilot, Codex, et autres). Il optimise l'utilisation des tokens en pr√©processant les donn√©es via des scripts Bash et organise les commentaires par PR et par niveau d'importance.

## Processus

### 1. Collecte des Donn√©es

Le skill utilise le script `collect-pr-data.sh` pour :
- Identifier le repository courant via `gh repo view`
- R√©cup√©rer la liste des PR en cours avec `gh pr list`
- Pour chaque PR, extraire les m√©tadonn√©es compl√®tes
- T√©l√©charger tous les commentaires de review

### 2. Extraction des M√©tadonn√©es des Agents IA

Via `parse-review-agents.sh`, le skill :
- Identifie les commentaires provenant des agents IA (CodeRabbit, Copilot, Codex, etc.)
- Extrait les m√©tadonn√©es de classification (‚ö†Ô∏è Potential issue, üü† Major, etc.)
- Classe les commentaires par agent, type et importance
- Cr√©e une structure organis√©e : PR > Importance > Commentaire individuel
- Architecture extensible pour supporter de nouveaux agents

### 3. G√©n√©ration de R√©sum√©s

Le script `generate-summary.sh` produit :
- Validation de chaque √©tape du processus avec indicateurs visuels
- Statistiques concises par PR et globales
- Fichier `summary.md` par PR avec m√©triques essentielles
- Rapport global `global-summary.md` avec vue d'ensemble

## Utilisation

### D√©clencheurs Typiques
- "Analyse les PR en cours de ce repository"
- "Que dit CodeRabbit sur les derni√®res PR ?"
- "Donne-moi un r√©sum√© des reviews des PR ouvertes"
- "Quels sont les probl√®mes identifi√©s par CodeRabbit ?"

### Sortie

Les donn√©es sont stock√©es dans `.scd/github-pr-collector/data/pr-data/` et un r√©sum√© est affich√© √† l'utilisateur avec :
- Nombre de PR analys√©es
- Distribution des commentaires par s√©v√©rit√©
- Statistiques par PR
- Lien vers les fichiers d√©taill√©s g√©n√©r√©s

## Gestion des Erreurs

Le skill g√®re gracieusement :
- L'absence de GitHub CLI ou d'authentification
- Les repositories sans PR
- Les PR sans commentaires CodeRabbit
- Les limites de taux de l'API GitHub

## R√©f√©rence

Les scripts utilisent les ressources suivantes :
- `scripts/collect-pr-data.sh` - Collection GitHub CLI
- `scripts/parse-coderabbit.sh` - Parsing jq des m√©tadonn√©es
- `scripts/generate-summary.sh` - G√©n√©ration Markdown
```

#### **Script collect-pr-data.sh**

```bash
#!/bin/bash
set -euo pipefail

# Script de collecte des donn√©es GitHub PR avec gestion CodeRabbit
# Auteur: scd-cc
# Version: 1.0.0

# Configuration et variables globales
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
readonly DATA_DIR="${PROJECT_ROOT}/.scd"
readonly PR_DATA_DIR="${DATA_DIR}/pr-data"
readonly CONFIG_DIR="${DATA_DIR}/config"
readonly CACHE_DIR="${DATA_DIR}/cache"
readonly LOG_FILE="${DATA_DIR}/collect-pr.log"

# Couleurs pour l'affichage
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

# Fonction de logging
log() {
    local level="$1"
    shift
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] [$level] $*" | tee -a "$LOG_FILE"
}

# Fonction de nettoyage
cleanup() {
    local exit_code=$?
    if [[ -d "$CACHE_DIR" ]]; then
        rm -rf "${CACHE_DIR:?}"/*
    fi
    exit $exit_code
}

# Configuration du trap pour le nettoyage
trap cleanup EXIT INT TERM

# V√©rification des pr√©requis
check_prerequisites() {
    log "INFO" "V√©rification des pr√©requis..."
    
    if ! command -v gh >/dev/null 2>&1; then
        log "ERROR" "GitHub CLI (gh) n'est pas install√©"
        return 1
    fi
    
    if ! command -v jq >/dev/null 2>&1; then
        log "ERROR" "jq n'est pas install√©"
        return 1
    fi
    
    # V√©rification de l'authentification GitHub
    if ! gh auth status >/dev/null 2>&1; then
        log "ERROR" "Non authentifi√© avec GitHub CLI. Ex√©cutez: gh auth login"
        return 1
    fi
    
    log "INFO" "Pr√©requis valid√©s ‚úì"
    return 0
}

# Cr√©ation de la structure de dossiers
setup_directories() {
    log "INFO" "Configuration des r√©pertoires..."
    
    mkdir -p "$PR_DATA_DIR" "$CACHE_DIR"
    
    # Permissions s√©curis√©es
    chmod 750 "$DATA_DIR" "$PR_DATA_DIR" "$CACHE_DIR"
    
    log "INFO" "Structure cr√©√©e: $PR_DATA_DIR"
}

# R√©cup√©ration des informations du repository
get_repo_info() {
    log "INFO" "R√©cup√©ration des informations du repository..."
    
    local repo_info
    if ! repo_info=$(gh repo view --json nameWithOwner,defaultBranchRef 2>/dev/null); then
        log "ERROR" "Impossible de r√©cup√©rer les informations du repository"
        return 1
    fi
    
    echo "$repo_info" | jq -r '.nameWithOwner'
}

# Collecte des Pull Requests
collect_pull_requests() {
    local repo_name="$1"
    local pr_state="${2:-open}"
    
    log "INFO" "Collecte des PR ($pr_state) pour $repo_name..."
    
    local pr_list_file="${CACHE_DIR}/pr-list.json"
    
    # R√©cup√©ration de la liste des PR avec toutes les m√©tadonn√©es n√©cessaires
    if ! gh pr list \
        --repo "$repo_name" \
        --state "$pr_state" \
        --json number,title,author,createdAt,updatedAt,url,headRefName,baseRefName,draft,mergeable \
        --limit 50 > "$pr_list_file"; then
        log "ERROR" "√âchec de la r√©cup√©ration des PR"
        return 1
    fi
    
    local pr_count
    pr_count=$(jq 'length' "$pr_list_file")
    
    if [[ "$pr_count" -eq 0 ]]; then
        log "WARN" "Aucune PR trouv√©e dans l'√©tat: $pr_state"
        return 0
    fi
    
    log "INFO" "Trouv√© $pr_count PR(s) √† analyser"
    
    # Traitement de chaque PR
    local pr_number
    while IFS= read -r pr_number; do
        if [[ -n "$pr_number" ]] && [[ "$pr_number" != "null" ]]; then
            process_pull_request "$repo_name" "$pr_number"
        fi
    done < <(jq -r '.[].number' "$pr_list_file")
    
    return 0
}

# Traitement d'une Pull Request individuelle
process_pull_request() {
    local repo_name="$1"
    local pr_number="$2"
    
    log "INFO" "Traitement de la PR #$pr_number..."
    
    local pr_data_file="${CACHE_DIR}/pr-${pr_number}-data.json"
    local pr_reviews_file="${CACHE_DIR}/pr-${pr_number}-reviews.json"
    
    # R√©cup√©ration des donn√©es d√©taill√©es de la PR
    if ! gh pr view "$pr_number" \
        --repo "$repo_name" \
        --json number,title,body,author,createdAt,updatedAt,url,headRefName,baseRefName,draft,mergeable,labels,assignees,reviewRequests,milestone,projectCards \
        > "$pr_data_file"; then
        log "WARN" "√âchec de la r√©cup√©ration des donn√©es pour PR #$pr_number"
        return 1
    fi
    
    # R√©cup√©ration des reviews et commentaires
    if ! gh api "repos/$repo_name/pulls/$pr_number/reviews" \
        --paginate \
        --jq '.[] | select(.body != null and .body != "")' \
        > "$pr_reviews_file"; then
        log "WARN" "√âchec de la r√©cup√©ration des reviews pour PR #$pr_number"
        # Continuer m√™me sans reviews
        echo "[]" > "$pr_reviews_file"
    fi
    
    # R√©cup√©ration des commentaires de review (commentaires sur les lignes de code)
    local pr_review_comments_file="${CACHE_DIR}/pr-${pr_number}-review-comments.json"
    if ! gh api "repos/$repo_name/pulls/$pr_number/comments" \
        --paginate \
        > "$pr_review_comments_file"; then
        log "WARN" "√âchec de la r√©cup√©ration des commentaires de review pour PR #$pr_number"
        echo "[]" > "$pr_review_comments_file"
    fi
    
    # Appel du script de parsing des agents de review
    if ! "$SCRIPT_DIR/parse-review-agents.sh" "$pr_number" "$pr_data_file" "$pr_reviews_file" "$pr_review_comments_file"; then
        log "ERROR" "√âchec du parsing des agents de review pour PR #$pr_number"
        return 1
    fi
    
    log "INFO" "PR #$pr_number trait√©e avec succ√®s ‚úì"
    return 0
}

# G√©n√©ration du rapport global
generate_global_report() {
    log "INFO" "G√©n√©ration du rapport global..."
    
    local report_file="${PR_DATA_DIR}/pr-analysis-report.md"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    cat > "$report_file" << EOF
# Rapport d'Analyse des Pull Requests

**G√©n√©r√© le :** $timestamp
**Repository :** $(get_repo_info)

## R√©sum√©

EOF
    
    # Comptage des fichiers g√©n√©r√©s
    local pr_count=0
    for file in "$PR_DATA_DIR"/pr-*-summary.md; do
        [[ -f "$file" ]] && ((pr_count++))
    done
    
    echo "- **Pull Requests analys√©es :** $pr_count" >> "$report_file"
    echo "- **Donn√©es stock√©es dans :** \`.scd/github-pr-collector/data/pr-data/\`" >> "$report_file"
    echo "" >> "$report_file"
    
    if [[ $pr_count -gt 0 ]]; then
        echo "## Liste des Pull Requests" >> "$report_file"
        echo "" >> "$report_file"
        
        for file in "$PR_DATA_DIR"/pr-*-summary.md; do
            if [[ -f "$file" ]]; then
                local pr_num=$(basename "$file" | sed 's/pr-\([0-9]*\)-summary\.md/\1/')
                local pr_title=$(head -n 1 "$file" | sed 's/^# //')
                echo "- [PR #$pr_num: $pr_title]($file)" >> "$report_file"
            fi
        done
    fi
    
    log "INFO" "Rapport global g√©n√©r√©: $report_file"
    echo -e "${GREEN}‚úì Rapport g√©n√©r√©: $report_file${NC}"
}

# Fonction principale
main() {
    local pr_state="${1:-open}"
    
    echo -e "${BLUE}üöÄ GitHub PR Collector - D√©marrage${NC}"
    
    # V√©rifications pr√©liminaires
    if ! check_prerequisites; then
        exit 1
    fi
    
    # Configuration
    setup_directories
    
    # R√©cup√©ration du nom du repository
    local repo_name
    if ! repo_name=$(get_repo_info); then
        log "ERROR" "Impossible de d√©terminer le repository courant"
        exit 1
    fi
    
    echo -e "${BLUE}üìä Repository: $repo_name${NC}"
    
    # Collecte des PR
    if ! collect_pull_requests "$repo_name" "$pr_state"; then
        log "ERROR" "√âchec de la collecte des PR"
        exit 1
    fi
    
    # G√©n√©ration du rapport global
    generate_global_report
    
    echo -e "${GREEN}‚úÖ Collecte termin√©e avec succ√®s!${NC}"
    echo -e "${YELLOW}üìÇ Donn√©es disponibles dans: $PR_DATA_DIR${NC}"
    
    log "INFO" "Collecte GitHub PR termin√©e avec succ√®s"
}

# Ex√©cution si appel√© directement
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
```

#### **Script parse-review-agents.sh**

```bash
#!/bin/bash
set -euo pipefail

# Script de parsing des m√©tadonn√©es des agents de review IA
# Auteur: scd-cc
# Version: 1.0.0

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
readonly DATA_DIR="${PROJECT_ROOT}/.scd"
readonly PR_DATA_DIR="${DATA_DIR}/pr-data"
readonly CONFIG_DIR="${DATA_DIR}/config"

# Configuration des agents de review
readonly AGENTS_PATTERNS_FILE="${CONFIG_DIR}/agents-patterns.json"
readonly SEVERITY_MAPPING_FILE="${CONFIG_DIR}/severity-mapping.json"

# Fonction de logging
log() {
    local level="$1"
    shift
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] [$level] $*"
}

# Initialisation des patterns des agents de review
init_review_agents_patterns() {
    mkdir -p "$CONFIG_DIR"
    
    if [[ ! -f "$AGENTS_PATTERNS_FILE" ]]; then
        cat > "$AGENTS_PATTERNS_FILE" << 'EOF'
{
  "agents": {
    "coderabbit": {
      "indicators": ["coderabbitai", "CodeRabbit", "AI-generated review"],
      "author_patterns": ["coderabbitai", "coderabbit"]
    },
    "github_copilot": {
      "indicators": ["GitHub Copilot", "Copilot review", "copilot-chat"],
      "author_patterns": ["github-copilot", "copilot"]
    },
    "codex": {
      "indicators": ["OpenAI Codex", "Codex review", "GPT-4"],
      "author_patterns": ["codex", "openai-codex"]
    },
    "generic_ai": {
      "indicators": ["AI review", "Automated review", "Bot review"],
      "author_patterns": ["bot", "ai-", "automated"]
    }
  },
  "severity_patterns": {
    "critical": {
      "emoji": ["üî¥", "‚ùå", "üö®", "‚õî"],
      "keywords": ["Critical", "CRITICAL", "Severe", "Security", "Vulnerability", "Breaking"]
    },
    "major": {
      "emoji": ["üü†", "‚ö†Ô∏è", "üü°"],
      "keywords": ["Major", "MAJOR", "Important", "Issue", "Problem", "Bug", "Error"]
    },
    "minor": {
      "emoji": ["üü°", "üí°", "‚ö°"],
      "keywords": ["Minor", "MINOR", "Warning", "Suggestion", "Potential issue", "Improvement"]
    },
    "trivial": {
      "emoji": ["üîµ", "‚ÑπÔ∏è", "ÔøΩ", "‚ú®"],
      "keywords": ["Info", "INFO", "Note", "Consider", "Tip", "Enhancement", "Style", "Format"]
    }
  },
  "category_patterns": {
    "performance": ["Performance", "Optimization", "Efficiency", "Speed", "Memory"],
    "security": ["Security", "Vulnerability", "Authentication", "Authorization", "XSS", "SQL"],
    "maintainability": ["Maintainability", "Readability", "Code quality", "Refactor", "Clean"],
    "testing": ["Test", "Testing", "Coverage", "Assert", "Mock", "Unit test"],
    "documentation": ["Documentation", "Comment", "README", "Docs", "JSDoc"],
    "accessibility": ["Accessibility", "A11y", "ARIA", "Screen reader"],
    "type_safety": ["Type", "TypeScript", "Interface", "Generic"]
  }
}
EOF
        log "INFO" "Patterns des agents initialis√©s: $AGENTS_PATTERNS_FILE"
    fi
    
    if [[ ! -f "$SEVERITY_MAPPING_FILE" ]]; then
        cat > "$SEVERITY_MAPPING_FILE" << 'EOF'
{
  "severity_order": ["critical", "major", "minor", "trivial"],
  "folder_mapping": {
    "critical": "üî¥-critical",
    "major": "üü†-major", 
    "minor": "üü°-minor",
    "trivial": "üîµ-trivial"
  },
  "default_severity": "trivial"
}
EOF
        log "INFO" "Mapping de s√©v√©rit√© initialis√©: $SEVERITY_MAPPING_FILE"
    fi
}

# Identification de l'agent de review
identify_review_agent() {
    local comment_text="$1"
    local author="$2"
    
    # Parcours de tous les agents configur√©s
    local agents
    agents=$(jq -r '.agents | keys[]' "$AGENTS_PATTERNS_FILE")
    
    while IFS= read -r agent; do
        # V√©rification par auteur
        local author_patterns
        author_patterns=$(jq -r ".agents.$agent.author_patterns[]?" "$AGENTS_PATTERNS_FILE" 2>/dev/null || echo "")
        
        while IFS= read -r pattern; do
            if [[ -n "$pattern" ]] && echo "$author" | grep -iq "$pattern"; then
                echo "$agent"
                return 0
            fi
        done <<< "$author_patterns"
        
        # V√©rification par contenu
        local indicators
        indicators=$(jq -r ".agents.$agent.indicators[]?" "$AGENTS_PATTERNS_FILE" 2>/dev/null || echo "")
        
        while IFS= read -r indicator; do
            if [[ -n "$indicator" ]] && echo "$comment_text" | grep -iq "$indicator"; then
                echo "$agent"
                return 0
            fi
        done <<< "$indicators"
    done <<< "$agents"
    
    # Aucun agent identifi√©
    echo "unknown"
    return 1
}

# Classification de la s√©v√©rit√© d'un commentaire
classify_severity() {
    local comment_text="$1"
    
    # Ordre de priorit√© des s√©v√©rit√©s
    local severities
    severities=$(jq -r '.severity_order[]' "$SEVERITY_MAPPING_FILE")
    
    while IFS= read -r severity; do
        # V√©rification des emojis
        local emojis
        emojis=$(jq -r ".severity_patterns.$severity.emoji[]?" "$AGENTS_PATTERNS_FILE" 2>/dev/null || echo "")
        
        while IFS= read -r emoji; do
            if [[ -n "$emoji" ]] && echo "$comment_text" | grep -F "$emoji" >/dev/null; then
                echo "$severity"
                return 0
            fi
        done <<< "$emojis"
        
        # V√©rification des mots-cl√©s
        local keywords
        keywords=$(jq -r ".severity_patterns.$severity.keywords[]?" "$AGENTS_PATTERNS_FILE" 2>/dev/null || echo "")
        
        while IFS= read -r keyword; do
            if [[ -n "$keyword" ]] && echo "$comment_text" | grep -iw "$keyword" >/dev/null; then
                echo "$severity"
                return 0
            fi
        done <<< "$keywords"
    done <<< "$severities"
    
    # S√©v√©rit√© par d√©faut
    jq -r '.default_severity' "$SEVERITY_MAPPING_FILE"
}

# Classification de la cat√©gorie d'un commentaire
classify_category() {
    local comment_text="$1"
    
    # Parcours de toutes les cat√©gories configur√©es
    local categories
    categories=$(jq -r '.category_patterns | keys[]' "$AGENTS_PATTERNS_FILE")
    
    while IFS= read -r category; do
        local keywords
        keywords=$(jq -r ".category_patterns.$category[]?" "$AGENTS_PATTERNS_FILE" 2>/dev/null || echo "")
        
        while IFS= read -r keyword; do
            if [[ -n "$keyword" ]] && echo "$comment_text" | grep -iw "$keyword" >/dev/null; then
                echo "$category"
                return 0
            fi
        done <<< "$keywords"
    done <<< "$categories"
    
    echo "general"  # Cat√©gorie par d√©faut
}

# Extraction et traitement d'un commentaire
process_comment() {
    local comment_json="$1"
    local comment_id="$2"
    
    local author body created_at path line url
    author=$(echo "$comment_json" | jq -r '.user.login // .author.login // "unknown"')
    body=$(echo "$comment_json" | jq -r '.body // ""')
    created_at=$(echo "$comment_json" | jq -r '.created_at // .submitted_at // ""')
    path=$(echo "$comment_json" | jq -r '.path // ""')
    line=$(echo "$comment_json" | jq -r '.line // .original_line // ""')
    url=$(echo "$comment_json" | jq -r '.html_url // ""')
    
    # Identification de l'agent de review
    local agent
    agent=$(identify_review_agent "$body" "$author")
    
    if [[ "$agent" == "unknown" ]]; then
        return 1  # Pas un commentaire d'agent de review
    fi
    
    # Classification
    local severity category
    severity=$(classify_severity "$body")
    category=$(classify_category "$body")
    
    # Construction de l'objet JSON enrichi
    jq -n \
        --arg id "$comment_id" \
        --arg agent "$agent" \
        --arg author "$author" \
        --arg body "$body" \
        --arg created_at "$created_at" \
        --arg path "$path" \
        --arg line "$line" \
        --arg url "$url" \
        --arg severity "$severity" \
        --arg category "$category" \
        '{
            id: $id,
            agent: $agent,
            author: $author,
            body: $body,
            created_at: $created_at,
            path: $path,
            line: $line,
            url: $url,
            severity: $severity,
            category: $category,
            is_ai_review: true
        }'
}

# Parsing principal d'une PR avec structure organis√©e
parse_pr_review_agents() {
    local pr_number="$1"
    local pr_data_file="$2"
    local pr_reviews_file="$3"
    local pr_review_comments_file="$4"
    
    log "INFO" "Parsing des agents de review pour PR #$pr_number..."
    
    # Cr√©ation de la structure de dossiers pour cette PR
    local pr_dir="${PR_DATA_DIR}/pr-${pr_number}"
    mkdir -p "$pr_dir"
    
    # Cr√©ation des dossiers par importance
    local severity_folders
    severity_folders=$(jq -r '.folder_mapping | to_entries[] | .value' "$SEVERITY_MAPPING_FILE")
    while IFS= read -r folder; do
        mkdir -p "${pr_dir}/${folder}"
    done <<< "$severity_folders"
    
    local all_comments="[]"
    local comment_counter=1
    
    # Traitement des reviews
    if [[ -f "$pr_reviews_file" ]]; then
        while IFS= read -r review; do
            local review_id
            review_id=$(echo "$review" | jq -r '.id // "unknown"')
            
            if processed_comment=$(process_comment "$review" "review-${review_id}" 2>/dev/null); then
                all_comments=$(echo "$all_comments" | jq ". + [$processed_comment]")
                save_individual_comment "$pr_dir" "$processed_comment" "$comment_counter"
                ((comment_counter++))
            fi
        done < <(jq -c '.[]?' "$pr_reviews_file" 2>/dev/null || echo "")
    fi
    
    # Traitement des commentaires de review
    if [[ -f "$pr_review_comments_file" ]]; then
        while IFS= read -r comment; do
            local comment_id
            comment_id=$(echo "$comment" | jq -r '.id // "unknown"')
            
            if processed_comment=$(process_comment "$comment" "comment-${comment_id}" 2>/dev/null); then
                all_comments=$(echo "$all_comments" | jq ". + [$processed_comment]")
                save_individual_comment "$pr_dir" "$processed_comment" "$comment_counter"
                ((comment_counter++))
            fi
        done < <(jq -c '.[]?' "$pr_review_comments_file" 2>/dev/null || echo "")
    fi
    
    # Tri par s√©v√©rit√©
    local sorted_comments
    local severity_order
    severity_order=$(jq -r '.severity_order | join(",")' "$SEVERITY_MAPPING_FILE")
    
    sorted_comments=$(echo "$all_comments" | jq --arg order "$severity_order" '
        def severity_to_num($sev; $order):
            ($order | split(",") | to_entries | map(select(.value == $sev)) | .[0].key // 999);
        sort_by(severity_to_num(.severity; $order))')
    
    # Sauvegarde du fichier de donn√©es compl√®tes
    echo "$sorted_comments" > "${pr_dir}/data.json"
    
    # G√©n√©ration du r√©sum√©
    generate_pr_summary "$pr_number" "$pr_data_file" "$sorted_comments" "$pr_dir"
    
    local comment_count
    comment_count=$(echo "$sorted_comments" | jq 'length')
    log "INFO" "PR #$pr_number: $comment_count commentaires d'agents identifi√©s"
    
    return 0
}

# Sauvegarde d'un commentaire individuel dans le bon dossier
save_individual_comment() {
    local pr_dir="$1"
    local comment_json="$2"
    local counter="$3"
    
    local severity agent id
    severity=$(echo "$comment_json" | jq -r '.severity')
    agent=$(echo "$comment_json" | jq -r '.agent')
    id=$(echo "$comment_json" | jq -r '.id')
    
    # R√©cup√©ration du nom du dossier correspondant √† la s√©v√©rit√©
    local severity_folder
    severity_folder=$(jq -r ".folder_mapping.${severity}" "$SEVERITY_MAPPING_FILE")
    
    local comment_file="${pr_dir}/${severity_folder}/comment-${counter}-${agent}-${id}.md"
    
    # G√©n√©ration du fichier Markdown du commentaire
    cat > "$comment_file" << EOF
# Commentaire ${counter} - ${agent^}

## M√©tadonn√©es

- **ID:** ${id}
- **Agent:** ${agent}
- **S√©v√©rit√©:** ${severity}
- **Cat√©gorie:** $(echo "$comment_json" | jq -r '.category')
- **Auteur:** $(echo "$comment_json" | jq -r '.author')
- **Date:** $(echo "$comment_json" | jq -r '.created_at')
- **Fichier:** $(echo "$comment_json" | jq -r '.path')
- **Ligne:** $(echo "$comment_json" | jq -r '.line')
- **URL:** $(echo "$comment_json" | jq -r '.url')

## Commentaire

$(echo "$comment_json" | jq -r '.body')

---
*G√©n√©r√© automatiquement par scd-cc*
EOF
}

# G√©n√©ration du r√©sum√© Markdown pour une PR
generate_pr_summary() {
    local pr_number="$1"
    local pr_data_file="$2"
    local sorted_comments="$3"
    local pr_dir="$4"
    
    local summary_file="${pr_dir}/summary.md"
    
    # Extraction des m√©tadonn√©es de base de la PR
    local pr_title pr_author pr_url pr_created
    pr_title=$(jq -r '.title // "Titre non disponible"' "$pr_data_file")
    pr_author=$(jq -r '.author.login // "Auteur inconnu"' "$pr_data_file")
    pr_url=$(jq -r '.url // ""' "$pr_data_file")
    pr_created=$(jq -r '.createdAt // ""' "$pr_data_file")
    
    # Statistiques des commentaires
    local total_comments critical_count major_count minor_count trivial_count
    total_comments=$(echo "$sorted_comments" | jq 'length')
    critical_count=$(echo "$sorted_comments" | jq '[.[] | select(.severity == "critical")] | length')
    major_count=$(echo "$sorted_comments" | jq '[.[] | select(.severity == "major")] | length')
    minor_count=$(echo "$sorted_comments" | jq '[.[] | select(.severity == "minor")] | length')
    trivial_count=$(echo "$sorted_comments" | jq '[.[] | select(.severity == "trivial")] | length')
    
    # Statistiques par agent
    local agents_stats
    agents_stats=$(echo "$sorted_comments" | jq -r 'group_by(.agent) | map({agent: .[0].agent, count: length}) | sort_by(-.count) | .[] | "\(.agent): \(.count)"')
    
    # G√©n√©ration du r√©sum√© concis
    cat > "$summary_file" << EOF
# PR #$pr_number: $pr_title

## üìä Statistiques Rapides

| M√©trique | Valeur |
|----------|---------|
| **Auteur** | $pr_author |
| **Date** | $pr_created |
| **Total commentaires** | $total_comments |
| **üî¥ Critiques** | $critical_count |
| **üü† Majeurs** | $major_count |
| **üü° Mineurs** | $minor_count |
| **üîµ Triviaux** | $trivial_count |

## ü§ñ Agents de Review

EOF
    
    while IFS= read -r agent_stat; do
        echo "- $agent_stat" >> "$summary_file"
    done <<< "$agents_stats"
    
    cat >> "$summary_file" << EOF

## üîó Navigation

- **URL de la PR:** $pr_url
- **Commentaires critiques:** [üî¥-critical/](./üî¥-critical/)
- **Commentaires majeurs:** [üü†-major/](./üü†-major/)
- **Commentaires mineurs:** [üü°-minor/](./üü°-minor/)
- **Commentaires triviaux:** [üîµ-trivial/](./üîµ-trivial/)

---
*G√©n√©r√© le $(date '+%Y-%m-%d %H:%M:%S') par scd-cc*
EOF
    
    log "INFO" "R√©sum√© g√©n√©r√©: $summary_file"
}

# G√©n√©ration du fichier d√©taill√© des reviews
generate_detailed_reviews() {
    local pr_number="$1"
    local parsed_file="$2"
    local reviews_file="$3"
    
    cat > "$reviews_file" << EOF
# PR #$pr_number - Reviews CodeRabbit D√©taill√©es

## Commentaires par S√©v√©rit√©

EOF
    
    for severity in critical major minor info; do
        local severity_count
        severity_count=$(jq "[.[] | select(.severity == \"$severity\")] | length" "$parsed_file")
        
        if [[ $severity_count -gt 0 ]]; then
            local severity_emoji
            case $severity in
                critical) severity_emoji="üî¥" ;;
                major) severity_emoji="üü†" ;;
                minor) severity_emoji="üü°" ;;
                info) severity_emoji="üîµ" ;;
            esac
            
            echo "### $severity_emoji ${severity^} ($severity_count)" >> "$reviews_file"
            echo "" >> "$reviews_file"
            
            jq -r ".[] | select(.severity == \"$severity\") | 
                   \"**Fichier:** \(.path):\(.line)\\n\\n\(.body)\\n\\n---\\n\"" \
                   "$parsed_file" >> "$reviews_file"
        fi
    done
    
    log "INFO" "Reviews d√©taill√©es g√©n√©r√©es: $reviews_file"
}

# Fonction principale
main() {
    local pr_number="$1"
    local pr_data_file="$2"
    local pr_reviews_file="$3"
    local pr_review_comments_file="$4"
    
    # Initialisation
    init_review_agents_patterns
    
    # Parsing
    if ! parse_pr_review_agents "$pr_number" "$pr_data_file" "$pr_reviews_file" "$pr_review_comments_file"; then
        log "ERROR" "√âchec du parsing pour PR #$pr_number"
        return 1
    fi
    
    return 0
}

# Ex√©cution si appel√© directement
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    if [[ $# -ne 4 ]]; then
        echo "Usage: $0 <pr_number> <pr_data_file> <pr_reviews_file> <pr_review_comments_file>"
        exit 1
    fi
    
    main "$@"
fi
```

#### **Script generate-summary.sh**

```bash
#!/bin/bash
set -euo pipefail

# Script de g√©n√©ration de r√©sum√© concis avec validation d'√©tapes
# Auteur: scd-cc
# Version: 1.0.0

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
readonly DATA_DIR="${PROJECT_ROOT}/.scd"
readonly PR_DATA_DIR="${DATA_DIR}/pr-data"

# Couleurs pour l'affichage
readonly GREEN='\033[0;32m'
readonly BLUE='\033[0;34m'
readonly YELLOW='\033[1;33m'
readonly NC='\033[0m'

# Fonction de validation avec indicateur visuel
validate_step() {
    local step_name="$1"
    local validation_command="$2"
    
    echo -n "‚è≥ $step_name... "
    
    if eval "$validation_command" >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ${NC}"
        return 0
    else
        echo -e "‚ùå"
        return 1
    fi
}

# G√©n√©ration des statistiques globales
generate_global_stats() {
    local global_summary="${PR_DATA_DIR}/global-summary.md"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    echo -e "${BLUE}üìä G√©n√©ration des statistiques globales...${NC}"
    
    # Comptage des PR
    local pr_count=0
    local total_comments=0
    local critical_total=0
    local major_total=0
    local minor_total=0
    local trivial_total=0
    
    for pr_dir in "$PR_DATA_DIR"/pr-*/; do
        if [[ -d "$pr_dir" && -f "${pr_dir}data.json" ]]; then
            ((pr_count++))
            
            local pr_comments
            pr_comments=$(jq 'length' "${pr_dir}data.json")
            ((total_comments += pr_comments))
            
            local critical_count major_count minor_count trivial_count
            critical_count=$(jq '[.[] | select(.severity == "critical")] | length' "${pr_dir}data.json")
            major_count=$(jq '[.[] | select(.severity == "major")] | length' "${pr_dir}data.json")
            minor_count=$(jq '[.[] | select(.severity == "minor")] | length' "${pr_dir}data.json")
            trivial_count=$(jq '[.[] | select(.severity == "trivial")] | length' "${pr_dir}data.json")
            
            ((critical_total += critical_count))
            ((major_total += major_count))
            ((minor_total += minor_count))
            ((trivial_total += trivial_count))
        fi
    done
    
    # G√©n√©ration du rapport global
    cat > "$global_summary" << EOF
# üìã Rapport Global d'Analyse des PR

**G√©n√©r√© le :** $timestamp  
**Repository :** $(git config --get remote.origin.url 2>/dev/null | sed 's/.*\///' | sed 's/\.git$//' || "N/A")

## üéØ Vue d'Ensemble

| M√©trique | Valeur |
|----------|---------|
| **PR analys√©es** | $pr_count |
| **Total commentaires** | $total_comments |
| **üî¥ Critiques** | $critical_total |
| **üü† Majeurs** | $major_total |
| **üü° Mineurs** | $minor_total |
| **üîµ Triviaux** | $trivial_total |

## üìä R√©partition par S√©v√©rit√©

EOF
    
    if [[ $total_comments -gt 0 ]]; then
        local critical_pct major_pct minor_pct trivial_pct
        critical_pct=$(( (critical_total * 100) / total_comments ))
        major_pct=$(( (major_total * 100) / total_comments ))
        minor_pct=$(( (minor_total * 100) / total_comments ))
        trivial_pct=$(( (trivial_total * 100) / total_comments ))
        
        cat >> "$global_summary" << EOF
- üî¥ **Critiques :** $critical_pct% ($critical_total/$total_comments)
- üü† **Majeurs :** $major_pct% ($major_total/$total_comments)
- üü° **Mineurs :** $minor_pct% ($minor_total/$total_comments)
- üîµ **Triviaux :** $trivial_pct% ($trivial_total/$total_comments)

## üìÇ Pull Requests Analys√©es

EOF
        
        # Liste des PR avec liens
        for pr_dir in "$PR_DATA_DIR"/pr-*/; do
            if [[ -d "$pr_dir" && -f "${pr_dir}summary.md" ]]; then
                local pr_num
                pr_num=$(basename "$pr_dir" | sed 's/pr-//')
                local pr_title
                pr_title=$(head -n 1 "${pr_dir}summary.md" | sed 's/^# PR #[0-9]*: //')
                echo "- [PR #$pr_num: $pr_title](./pr-$pr_num/summary.md)" >> "$global_summary"
            fi
        done
    else
        echo "Aucun commentaire trouv√©." >> "$global_summary"
    fi
    
    cat >> "$global_summary" << EOF

---
*G√©n√©r√© automatiquement par scd-cc*
EOF
    
    echo -e "${GREEN}‚úÖ Rapport global cr√©√© : $global_summary${NC}"
}

# Validation de la structure des donn√©es
validate_data_structure() {
    validate_step "V√©rification du dossier .scd" "[[ -d '$DATA_DIR' ]]"
    validate_step "V√©rification du dossier pr-data" "[[ -d '$PR_DATA_DIR' ]]"
    
    local pr_found=false
    for pr_dir in "$PR_DATA_DIR"/pr-*/; do
        if [[ -d "$pr_dir" ]]; then
            pr_found=true
            break
        fi
    done
    
    validate_step "V√©rification des donn√©es PR" "$pr_found"
}

# Validation des fichiers individuels
validate_pr_files() {
    local validated_prs=0
    local total_prs=0
    
    for pr_dir in "$PR_DATA_DIR"/pr-*/; do
        if [[ -d "$pr_dir" ]]; then
            ((total_prs++))
            local pr_num
            pr_num=$(basename "$pr_dir" | sed 's/pr-//')
            
            local has_data has_summary has_structure
            has_data="[[ -f '${pr_dir}data.json' ]]"
            has_summary="[[ -f '${pr_dir}summary.md' ]]"
            has_structure="[[ -d '${pr_dir}üî¥-critical' && -d '${pr_dir}üü†-major' && -d '${pr_dir}üü°-minor' && -d '${pr_dir}üîµ-trivial' ]]"
            
            if validate_step "PR #$pr_num - Structure" "$has_structure" && \
               validate_step "PR #$pr_num - Donn√©es" "$has_data" && \
               validate_step "PR #$pr_num - R√©sum√©" "$has_summary"; then
                ((validated_prs++))
            fi
        fi
    done
    
    echo -e "${BLUE}üìà Validation: $validated_prs/$total_prs PR valid√©es${NC}"
    return 0
}

# Affichage des statistiques finales
display_final_stats() {
    echo ""
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    echo -e "${YELLOW}           R√âSUM√â D'EX√âCUTION          ${NC}"
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    
    local total_prs=0
    local total_comments=0
    local critical_count=0
    local major_count=0
    
    for pr_dir in "$PR_DATA_DIR"/pr-*/; do
        if [[ -d "$pr_dir" && -f "${pr_dir}data.json" ]]; then
            ((total_prs++))
            local pr_comments
            pr_comments=$(jq 'length' "${pr_dir}data.json")
            ((total_comments += pr_comments))
            
            local critical major
            critical=$(jq '[.[] | select(.severity == "critical")] | length' "${pr_dir}data.json")
            major=$(jq '[.[] | select(.severity == "major")] | length' "${pr_dir}data.json")
            ((critical_count += critical))
            ((major_count += major))
        fi
    done
    
    echo -e "üìä ${BLUE}Pull Requests analys√©es:${NC} $total_prs"
    echo -e "üí¨ ${BLUE}Commentaires trouv√©s:${NC} $total_comments"
    echo -e "üî¥ ${BLUE}Commentaires critiques:${NC} $critical_count"
    echo -e "üü† ${BLUE}Commentaires majeurs:${NC} $major_count"
    echo ""
    echo -e "üìÇ ${BLUE}Donn√©es stock√©es dans:${NC} $PR_DATA_DIR"
    echo -e "üìã ${BLUE}Rapport global:${NC} $PR_DATA_DIR/global-summary.md"
    echo -e "${YELLOW}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
}

# Fonction principale
main() {
    echo -e "${BLUE}üöÄ G√©n√©ration du r√©sum√© global - scd-cc${NC}"
    echo ""
    
    # Validation de la structure
    if ! validate_data_structure; then
        echo "‚ùå Erreur: Structure de donn√©es invalide"
        exit 1
    fi
    
    echo ""
    
    # Validation des fichiers PR
    validate_pr_files
    
    echo ""
    
    # G√©n√©ration des statistiques globales
    generate_global_stats
    
    echo ""
    
    # Affichage du r√©sum√© final
    display_final_stats
    
    echo ""
    echo -e "${GREEN}‚úÖ R√©sum√© g√©n√©r√© avec succ√®s!${NC}"
}

# Ex√©cution si appel√© directement
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
```

---

### **Skill 2: CodeRabbit Review Analyzer**

#### **SKILL.md - CodeRabbit Review Analyzer**

```yaml
---
name: "coderabbit-analyzer"
description: "Analyse les commentaires de review CodeRabbit pr√©alablement collect√©s et g√©n√®re des insights approfondis. Utilise les donn√©es structur√©es du dossier .scd pour fournir des analyses de tendances, des recommandations d'am√©lioration et des m√©triques de qualit√©. √Ä utiliser apr√®s avoir collect√© les donn√©es avec github-pr-collector."
version: "1.0.0"
dependencies:
  - "github-pr-collector >= 1.0.0"
---

# CodeRabbit Review Analyzer Skill

## Objectif

Ce skill analyse les donn√©es de review des agents IA pr√©alablement collect√©es et stock√©es dans `.scd/github-pr-collector/data/pr-data/` pour g√©n√©rer des insights approfondis, des tendances et des recommandations d'am√©lioration pour l'√©quipe de d√©veloppement.

## Processus d'Analyse

### 1. Analyse des Donn√©es Collect√©es

Le skill examine les fichiers g√©n√©r√©s par `github-pr-collector` :
- Lit les r√©sum√©s de PR (`pr-*-summary.md`)
- Parse les reviews d√©taill√©es (`pr-*-reviews.md`)
- Analyse les donn√©es JSON structur√©es (`pr-*-coderabbit.json`)

### 2. G√©n√©ration d'Insights

#### M√©triques de Qualit√©
- Distribution des s√©v√©rit√©s des commentaires
- Tendances par fichier/r√©pertoire
- Types de probl√®mes les plus fr√©quents
- √âvolution temporelle de la qualit√©

#### Analyses Comportementales
- Patterns de review r√©currents
- Cat√©gories de probl√®mes dominantes
- Impact des corrections sur les m√©triques

#### Recommandations
- Zones du code n√©cessitant plus d'attention
- Formations recommand√©es pour l'√©quipe
- Processus d'am√©lioration sugg√©r√©s

### 3. G√©n√©ration de Rapports

Le skill produit plusieurs types de rapports :
- **Rapport Ex√©cutif** : Vue d'ensemble pour le management
- **Rapport Technique** : Analyse d√©taill√©e pour les d√©veloppeurs
- **Plan d'Action** : Recommandations prioritaires

## Utilisation

### D√©clencheurs Typiques
- "Analyse les reviews CodeRabbit collect√©es"
- "Quelles sont les tendances des commentaires CodeRabbit ?"
- "G√©n√®re un rapport sur la qualit√© du code bas√© sur CodeRabbit"
- "Que nous apprennent les reviews CodeRabbit sur notre code ?"

### Pr√©requis
Les donn√©es doivent avoir √©t√© collect√©es au pr√©alable avec le skill `github-pr-collector`.

## Templates de Rapports

Le skill utilise des templates pr√©d√©finis dans `resources/analysis-templates.md` pour g√©n√©rer des rapports coh√©rents et professionnels.

## Personnalisation

L'analyse peut √™tre personnalis√©e via :
- Filtres par p√©riode
- Focus sur des cat√©gories sp√©cifiques
- Seuils de s√©v√©rit√© ajustables
- M√©triques personnalis√©es
```

#### **Resources - Analysis Templates**

```markdown
# Templates d'Analyse CodeRabbit

## Template Rapport Ex√©cutif

### Vue d'Ensemble de la Qualit√© du Code

**P√©riode d'analyse :** {DATE_RANGE}  
**Nombre de PR analys√©es :** {PR_COUNT}  
**Total des commentaires CodeRabbit :** {TOTAL_COMMENTS}

#### Indicateurs Cl√©s

| M√©trique | Valeur | Tendance |
|---------|---------|-----------|
| Commentaires Critiques | {CRITICAL_COUNT} | {CRITICAL_TREND} |
| Commentaires Majeurs | {MAJOR_COUNT} | {MAJOR_TREND} |
| Score de Qualit√© Global | {QUALITY_SCORE}/100 | {QUALITY_TREND} |

#### Cat√©gories Dominantes

1. **{TOP_CATEGORY_1}** : {TOP_CATEGORY_1_PERCENT}% des commentaires
2. **{TOP_CATEGORY_2}** : {TOP_CATEGORY_2_PERCENT}% des commentaires
3. **{TOP_CATEGORY_3}** : {TOP_CATEGORY_3_PERCENT}% des commentaires

#### Recommandations Prioritaires

{EXECUTIVE_RECOMMENDATIONS}

---

## Template Rapport Technique

### Analyse D√©taill√©e des Reviews CodeRabbit

#### Distribution par S√©v√©rit√©

```
üî¥ Critiques  : {CRITICAL_COUNT} ({CRITICAL_PERCENT}%)
üü† Majeurs    : {MAJOR_COUNT} ({MAJOR_PERCENT}%)
üü° Mineurs    : {MINOR_COUNT} ({MINOR_PERCENT}%)
üîµ Informatifs: {INFO_COUNT} ({INFO_PERCENT}%)
```

#### Analyse par Cat√©gorie

{CATEGORY_ANALYSIS}

#### Fichiers/R√©pertoires les Plus Comment√©s

{TOP_FILES_ANALYSIS}

#### Patterns R√©currents

{RECURRING_PATTERNS}

#### Recommandations Techniques

{TECHNICAL_RECOMMENDATIONS}

---

## Template Plan d'Action

### Plan d'Am√©lioration de la Qualit√© du Code

#### Actions Imm√©diates (Semaine 1-2)

{IMMEDIATE_ACTIONS}

#### Actions √† Moyen Terme (Mois 1-2)

{MEDIUM_TERM_ACTIONS}

#### Actions √† Long Terme (Trimestre)

{LONG_TERM_ACTIONS}

#### M√©triques de Suivi

{TRACKING_METRICS}
```

---

## **Scripts Bash S√©curis√©s**

### **Standards de S√©curit√© Appliqu√©s**

Tous les scripts suivent les **quatre piliers de s√©curisation** d√©finis dans votre documentation :

#### **1. Fondations Solides**
```bash
#!/bin/bash
set -euo pipefail  # Mode strict obligatoire

# Validation syst√©matique des entr√©es
validate_input() {
    local input="$1"
    local pattern="$2"
    
    if [[ ! "$input" =~ $pattern ]]; then
        log "ERROR" "Entr√©e invalide: $input"
        return 1
    fi
}

# Gestion des ressources temporaires
readonly TEMP_DIR=$(mktemp -d)
trap 'rm -rf "${TEMP_DIR}"' EXIT INT TERM
```

#### **2. Gestion des Secrets**
```bash
# V√©rification des variables d'environnement sensibles
check_github_auth() {
    if ! gh auth status >/dev/null 2>&1; then
        log "ERROR" "GitHub CLI non authentifi√©"
        echo "Ex√©cutez: gh auth login"
        return 1
    fi
}

# Nettoyage des logs pour √©viter l'exposition de tokens
sanitize_output() {
    sed -E 's/(token|key|secret)[[:space:]]*[:=][[:space:]]*[^[:space:]]+/\1=***REDACTED***/gi'
}
```

#### **3. Outillage Local**
Les scripts sont compatibles avec :
- **ShellCheck** : Analyse statique int√©gr√©e
- **Pre-commit hooks** : Validation automatique
- **VSCode extension** : Retour temps r√©el

#### **4. CI/CD GitHub Actions**
```yaml
# .github/workflows/shellcheck.yml
name: Shell Script Quality
on: [push, pull_request]
jobs:
  shellcheck:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: reviewdog/action-shellcheck@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          reporter: github-pr-review
```

---

## **Installation et D√©ploiement**

### **Script d'Installation install.sh**

```bash
#!/bin/bash
set -euo pipefail

# SCD-CC Installation Script
# Version: 1.0.0

readonly REPO_URL="https://github.com/negus/scd-cc"
readonly PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
readonly SKILLS_DIR="${PROJECT_ROOT}/.claude/skills"
readonly DATA_DIR="${PROJECT_ROOT}/.scd"

install_cc_skills() {
    echo "üöÄ Installation de SCD-CC dans le projet courant..."
    echo "üìÇ Projet d√©tect√©: $PROJECT_ROOT"
    
    # V√©rification des pr√©requis
    for cmd in gh jq curl; do
        if ! command -v "$cmd" >/dev/null 2>&1; then
            echo "‚ùå $cmd n'est pas install√©"
            exit 1
        fi
    done
    
    # V√©rification que nous sommes dans un projet Git
    if ! git rev-parse --git-dir >/dev/null 2>&1; then
        echo "‚ö†Ô∏è  Attention: Pas dans un d√©p√¥t Git. Installation dans le dossier courant."
    fi
    
    # Cr√©ation de la structure locale
    mkdir -p "$SKILLS_DIR" "$DATA_DIR"/{pr-data,config,cache}
    
    # T√©l√©chargement des skills depuis GitHub
    echo "üì¶ T√©l√©chargement des skills depuis $REPO_URL..."
    
    # T√©l√©chargement du skill github-pr-collector
    curl -fsSL "$REPO_URL/raw/main/skills/github-pr-collector/SKILL.md" \
        -o "$SKILLS_DIR/github-pr-collector.md"
    
    mkdir -p "$SKILLS_DIR/github-pr-collector/scripts"
    
    for script in collect-pr-data.sh parse-review-agents.sh generate-summary.sh; do
        curl -fsSL "$REPO_URL/raw/main/skills/github-pr-collector/scripts/$script" \
            -o "$SKILLS_DIR/github-pr-collector/scripts/$script"
        chmod +x "$SKILLS_DIR/github-pr-collector/scripts/$script"
    done
    
    # T√©l√©chargement du skill review-analyzer
    curl -fsSL "$REPO_URL/raw/main/skills/review-analyzer/SKILL.md" \
        -o "$SKILLS_DIR/review-analyzer.md"
    
    # Configuration des patterns par d√©faut
    curl -fsSL "$REPO_URL/raw/main/config/agents-patterns.json" \
        -o "$DATA_DIR/config/agents-patterns.json"
    
    curl -fsSL "$REPO_URL/raw/main/config/severity-mapping.json" \
        -o "$DATA_DIR/config/severity-mapping.json"
    
    # Ajout au .gitignore si existe
    if [[ -f "${PROJECT_ROOT}/.gitignore" ]]; then
        if ! grep -q "^\.scd/" "${PROJECT_ROOT}/.gitignore"; then
            echo "" >> "${PROJECT_ROOT}/.gitignore"
            echo "# SCD-CC data" >> "${PROJECT_ROOT}/.gitignore"
            echo ".scd/cache/" >> "${PROJECT_ROOT}/.gitignore"
            echo ".scd/*.log" >> "${PROJECT_ROOT}/.gitignore"
            echo "‚úÖ .gitignore mis √† jour"
        fi
    fi
    
    echo "‚úÖ Installation termin√©e!"
    echo "ÔøΩ Skills install√©s dans: $SKILLS_DIR"
    echo "üìä Donn√©es stock√©es dans: $DATA_DIR"
    echo ""
    echo "üöÄ Pour commencer:"
    echo "  1. Ouvrez Claude Code dans ce projet"
    echo "  2. Tapez: 'Analyse les PR en cours de ce repository'"
}

install_cc_skills
```

### **Installation en Une Ligne**

```bash
curl -fsSL https://raw.githubusercontent.com/negus/scd-cc/main/install/install.sh | bash
```

---

## **Utilisation et Workflows**

### **Workflow Type 1 : Analyse Ponctuelle**

```bash
# Dans Claude Code
"Analyse les PR en cours de ce repository avec CodeRabbit"

# Le skill github-pr-collector sera automatiquement invoqu√©
# Puis demander : "Maintenant analyse ces donn√©es"
# Le skill coderabbit-analyzer prendra le relais
```

### **Workflow Type 2 : Suivi Continu**

```bash
# Script cron pour collecte automatique
0 9 * * 1-5 /home/user/.scd-cc/skills/github-pr-collector/scripts/collect-pr-data.sh

# Analyse hebdomadaire dans Claude Code
"G√©n√®re le rapport hebdomadaire CodeRabbit"
```

### **Workflow Type 3 : Int√©gration CI/CD**

```yaml
# .github/workflows/coderabbit-analysis.yml
name: CodeRabbit Analysis
on:
  schedule:
    - cron: '0 6 * * 1'  # Lundi 6h
jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install SCD-CC
        run: curl -fsSL https://raw.githubusercontent.com/negus/scd-cc/main/install/install.sh | bash
      - name: Collect PR Data
        run: ~/.scd-cc/skills/github-pr-collector/scripts/collect-pr-data.sh
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

---

## **Maintenance et √âvolution**

### **Mise √† Jour des Patterns CodeRabbit**

Les patterns de d√©tection CodeRabbit √©voluent. Le fichier `.scd/config/coderabbit-patterns.json` peut √™tre mis √† jour :

```bash
# Mise √† jour manuelle
curl -fsSL https://raw.githubusercontent.com/negus/scd-cc/main/config/coderabbit-patterns.json \
  > ~/.scd-cc/.scd/config/coderabbit-patterns.json
```

### **Extensibilit√©**

Le syst√®me est con√ßu pour √™tre extensible :

1. **Nouveaux Parsers** : Ajouter support pour d'autres outils de review IA
2. **M√©triques Personnalis√©es** : √âtendre les analyses selon les besoins
3. **Int√©grations** : Connecter avec Slack, Jira, etc.

### **Monitoring et Logs**

```bash
# V√©rification des logs
tail -f ~/.scd-cc/.scd/collect-pr.log

# Nettoyage p√©riodique
find ~/.scd-cc/.scd/cache -type f -mtime +7 -delete
```

---

## **Conclusion**

Ce guide fournit un framework complet pour cr√©er des Skills Claude Code bas√©s sur Bash, optimis√©s pour l'analyse des Pull Requests GitHub avec CodeRabbit. 

### **Avantages de cette Approche**

1. **√âconomie de Tokens** : Les scripts Bash pr√©traitent les donn√©es
2. **S√©curit√© Renforc√©e** : Application des meilleures pratiques bash
3. **Maintenance Facilit√©e** : Structure modulaire et √©volutive
4. **Installation Simplifi√©e** : D√©ploiement en une commande
5. **Int√©gration Native** : Parfaitement adapt√© √† l'√©cosyst√®me GitHub

### **Prochaines √âtapes**

1. Cloner ce guide dans votre projet scd-cc
2. Adapter les patterns CodeRabbit √† votre contexte
3. Tester l'installation avec le script curl
4. Personnaliser les templates d'analyse
5. Int√©grer dans votre workflow CI/CD

Ce syst√®me transforme Claude Code en un analyste expert de vos Pull Requests, capable de fournir des insights pr√©cieux tout en optimisant l'utilisation des ressources IA.